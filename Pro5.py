# -*- coding: utf-8 -*-
"""Exercise 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m0rewA-U1Gx6qWTZARUoIuVBGPOEghOZ

# Read Data
"""

import numpy as np
import math
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')
data = pd.read_excel("drive/MyDrive/Colab Notebooks/exact.xlsx")
data = data.to_numpy()
R = data[:, 1]
J = data[:, 2]

"""# Model predicting"""

# Set all coefficient with a, b, c, d in order. At first, set all them to 0.
vectorCoefficient = np.zeros(4)
losses = [] # Cost function value storing
LogLosses = [] # Natural Logarit of Cost function value
il =[]
R_plot = [-2] #Storing value of predicting value of Romeo's love
J_plot = [3] #Storing value of predicting value of Juliet's love

for j in range (3500):
  ##initial gradient vector with gradient of a, b, c, d in order.
  vectorGradient = np.zeros(4)
  loss = 0

  predictR = -2
  predictJ = 3

  ##Explicit Euler Method and Gradient Descent Method
  for i in range(1000):
    
    tempR = predictR + (vectorCoefficient[0] * predictR + vectorCoefficient[1] * predictJ) * 0.001
    tempJ = predictJ + (vectorCoefficient[2] * predictR + vectorCoefficient[3] * predictJ) * 0.001

    ## Evaluate gradient value 
    vectorGradient[0] += predictR * 0.001 * (predictR - R[i]) #Gradient A
    vectorGradient[1] += predictJ * 0.001 * (predictR - R[i]) #Gradient B
    vectorGradient[2] += predictR * 0.001 * (predictJ - J[i]) #Gradient C
    vectorGradient[3] += predictJ * 0.001 * (predictJ - J[i]) #Gradient D
    loss += (predictR - R[i]) ** 2 + (predictJ - J[i]) ** 2 # Add up cost function

    predictR = tempR # Update predicted R for next loop 
    predictJ = tempJ # Update predicted J for next loop 
    if (j == 3499): # Checking if it is the last loop, which is the final value we predict
      R_plot.append(tempR) # Appending predicting value of R
      J_plot.append(tempJ) # Appending predicting value of J
    
  
  losses.append(loss/999)
  LogLosses.append(math.log(loss/999))
  vectorGradient/= 999
  vectorCoefficient -= 3.5 *vectorGradient
  
  il.append(j)

"""# Plot result"""

# Set up time for plotting later.
time = np.arange(start=0.001, stop=1.001, step=0.001)

#Print out the value of coefficients.
print("a =", vectorCoefficient[0])
print("b =", vectorCoefficient[1])
print("c =", vectorCoefficient[2])
print("d =", vectorCoefficient[3])

# Plot solutions

fig, axes = plt.subplots(2, 2, figsize = (8, 8))

#Plot exact value of R and J
axes[0,0].plot(time, R)
axes[0,0].plot(time, J)
axes[0,0].legend(["R predict value", "J predict value"])
axes[0,0].set_title("Predict value of R and J")

#Plot exact and predict value of R
axes[0,1].plot(time, R)
axes[0,1].plot(np.linspace(0, 1, 1001), R_plot)
axes[0,1].legend(["R exact value", "R predict value"])
axes[0,1].set_title("Exact and predict value of R")

#Plot exact and predict value of J
axes[1,0].plot(time, J)
axes[1,0].plot(np.linspace(0, 1, 1001), J_plot)
axes[1,0].legend(["J exact value", "J predict value"])
axes[1,0].set_title("Exact and predict value of J")

#Plot exact value of R and J
axes[1,1].plot(np.linspace(0, 1, 1001), R_plot)
axes[1,1].plot(np.linspace(0, 1, 1001), J_plot)
axes[1,1].legend(["R exact value", "J exact value"])
axes[1,1].set_title("Exact value of R and J")

lossValue = 0
for j in range(1, 1000, 1):
  lossValue += (R_plot[j] - R[j - 1]) ** 2 + (J_plot[j] - J[j - 1]) ** 2

lossValue /=999
print("Loss Value:", lossValue)

plt.plot(il, losses, 'y')
plt.grid()
plt.ylabel("Loss")
plt.show()

#Plot log(cost function value) to number of loop in gradient computation.
plt.plot(il, LogLosses, 'y')
plt.grid()
plt.ylabel("Loss")
plt.show()